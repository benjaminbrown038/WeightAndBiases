{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4JGxH4khGy4GFxU7BDK60",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjaminbrown038/WeightAndBiases/blob/main/notebooks/wandb/wandb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WANDB\n",
        "\n",
        "- Pytorch\n",
        "\n",
        "- Pytorch Lightning\n",
        "\n",
        "- Huggingface\n",
        "\n",
        "- Keras\n",
        "\n",
        "- XGBoost\n",
        "\n",
        "- LightGBM"
      ],
      "metadata": {
        "id": "b1S4y3T4gzTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch"
      ],
      "metadata": {
        "id": "NlwVpB86gzkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the library\n",
        "import wandb\n",
        "\n",
        "# start a new experiment\n",
        "wandb.init(project=\"new-sota-model\")\n",
        "\n",
        "#â€ƒcapture a dictionary of hyperparameters with config\n",
        "wandb.config = {\"learning_rate\": 0.001, \"epochs\": 100, \"batch_size\": 128}\n",
        "\n",
        "# set up model and data\n",
        "model, dataloader = get_model(), get_data()\n",
        "\n",
        "# optional: track gradients\n",
        "wandb.watch(model)\n",
        "\n",
        "for batch in dataloader:\n",
        "  metrics = model.training_step()\n",
        "  #â€ƒlog metrics inside your training loop to visualize model performance\n",
        "  wandb.log(metrics)\n",
        "\n",
        "# optional: save model at the end\n",
        "model.to_onnx()\n",
        "wandb.save(\"model.onnx\")"
      ],
      "metadata": {
        "id": "AnlY8XvTgzw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C103c7haiMdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Ensure deterministic behavior\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
        "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
        "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
        "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# remove slow mirror from list of MNIST mirrors\n",
        "torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors\n",
        "                                      if not mirror.startswith(\"http://yann.lecun.com\")]"
      ],
      "metadata": {
        "id": "MRezzAtKgz8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LFnrgKZ4g0BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb onnx -Uq"
      ],
      "metadata": {
        "id": "8y3bCeFZg0GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7ZEaXZWOg0LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "Zjpx45Bug0QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S4alnWoVg0ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(\n",
        "    epochs=5,\n",
        "    classes=10,\n",
        "    kernels=[16, 32],\n",
        "    batch_size=128,\n",
        "    learning_rate=0.005,\n",
        "    dataset=\"MNIST\",\n",
        "    architecture=\"CNN\")"
      ],
      "metadata": {
        "id": "7e0-KdWkg0lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F8LT8qjmg0pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_pipeline(hyperparameters):\n",
        "\n",
        "    # tell wandb to get started\n",
        "    with wandb.init(project=\"pytorch-demo\", config=hyperparameters):\n",
        "      # access all HPs through wandb.config, so logging matches execution!\n",
        "      config = wandb.config\n",
        "\n",
        "      # make the model, data, and optimization problem\n",
        "      model, train_loader, test_loader, criterion, optimizer = make(config)\n",
        "      print(model)\n",
        "\n",
        "      # and use them to train the model\n",
        "      train(model, train_loader, criterion, optimizer, config)\n",
        "\n",
        "      # and test its final performance\n",
        "      test(model, test_loader)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "_cuDZq1Og0uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XolmuVPVg086"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make(config):\n",
        "    # Make the data\n",
        "    train, test = get_data(train=True), get_data(train=False)\n",
        "    train_loader = make_loader(train, batch_size=config.batch_size)\n",
        "    test_loader = make_loader(test, batch_size=config.batch_size)\n",
        "\n",
        "    # Make the model\n",
        "    model = ConvNet(config.kernels, config.classes).to(device)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    return model, train_loader, test_loader, criterion, optimizer"
      ],
      "metadata": {
        "id": "9kCwEjB6g1B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vnkVhPIag1Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(slice=5, train=True):\n",
        "    full_dataset = torchvision.datasets.MNIST(root=\".\",\n",
        "                                              train=train,\n",
        "                                              transform=transforms.ToTensor(),\n",
        "                                              download=True)\n",
        "    #  equiv to slicing with [::slice]\n",
        "    sub_dataset = torch.utils.data.Subset(\n",
        "      full_dataset, indices=range(0, len(full_dataset), slice))\n",
        "\n",
        "    return sub_dataset\n",
        "\n",
        "\n",
        "def make_loader(dataset, batch_size):\n",
        "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=True,\n",
        "                                         pin_memory=True, num_workers=2)\n",
        "    return loader"
      ],
      "metadata": {
        "id": "Zqv3E3bFg1Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v1eZiBbxg1RS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conventional and convolutional neural network\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, kernels, classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, kernels[1], kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc = nn.Linear(7 * 7 * kernels[-1], classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "hBYg5fO8g10i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LRNXbaiRg18C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, criterion, optimizer, config):\n",
        "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "    # Run training and track with wandb\n",
        "    total_batches = len(loader) * config.epochs\n",
        "    example_ct = 0  # number of examples seen\n",
        "    batch_ct = 0\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        for _, (images, labels) in enumerate(loader):\n",
        "\n",
        "            loss = train_batch(images, labels, model, optimizer, criterion)\n",
        "            example_ct +=  len(images)\n",
        "            batch_ct += 1\n",
        "\n",
        "            # Report metrics every 25th batch\n",
        "            if ((batch_ct + 1) % 25) == 0:\n",
        "                train_log(loss, example_ct, epoch)\n",
        "\n",
        "\n",
        "def train_batch(images, labels, model, optimizer, criterion):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Forward pass âž¡\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass â¬…\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Step with optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Vq3rIUVyg2Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rerY4IY5g2Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_log(loss, example_ct, epoch):\n",
        "    # Where the magic happens\n",
        "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
        "    print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")"
      ],
      "metadata": {
        "id": "4H-yrsVAg2bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rQPquVoWhvGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"Accuracy of the model on the {total} \" +\n",
        "              f\"test images: {correct / total:%}\")\n",
        "\n",
        "        wandb.log({\"test_accuracy\": correct / total})\n",
        "\n",
        "    # Save the model in the exchangeable ONNX format\n",
        "    torch.onnx.export(model, images, \"model.onnx\")\n",
        "    wandb.save(\"model.onnx\")"
      ],
      "metadata": {
        "id": "rVwDvlj7hvLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DSWyJ8XGhvQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build, train and analyze the model with the pipeline\n",
        "model = model_pipeline(config)"
      ],
      "metadata": {
        "id": "JKcMxLLlhvWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch Lightning"
      ],
      "metadata": {
        "id": "HPbLJcPkhvak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning -q\n",
        "# install weights and biases\n",
        "!pip install wandb -qU"
      ],
      "metadata": {
        "id": "_53G9bZdhvgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_yQ5uLDJhvks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "# your favorite machine learning tracking tool\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "import wandb"
      ],
      "metadata": {
        "id": "9acguPeMhvp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CditA4Hghvul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "XZac_gbGhvzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FcRva6Bqhv4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size, data_dir: str = './'):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "        self.num_classes = 10\n",
        "\n",
        "    def prepare_data(self):\n",
        "        CIFAR10(self.data_dir, train=True, download=True)\n",
        "        CIFAR10(self.data_dir, train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == 'fit' or stage is None:\n",
        "            cifar_full = CIFAR10(self.data_dir, train=True, transform=self.transform)\n",
        "            self.cifar_train, self.cifar_val = random_split(cifar_full, [45000, 5000])\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == 'test' or stage is None:\n",
        "            self.cifar_test = CIFAR10(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.cifar_train, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.cifar_val, batch_size=self.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.cifar_test, batch_size=self.batch_size)"
      ],
      "metadata": {
        "id": "8IBKEloPhv89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nB9MUBrFiZVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImagePredictionLogger(pl.callbacks.Callback):\n",
        "    def __init__(self, val_samples, num_samples=32):\n",
        "        super().__init__()\n",
        "        self.num_samples = num_samples\n",
        "        self.val_imgs, self.val_labels = val_samples\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer, pl_module):\n",
        "        # Bring the tensors to CPU\n",
        "        val_imgs = self.val_imgs.to(device=pl_module.device)\n",
        "        val_labels = self.val_labels.to(device=pl_module.device)\n",
        "        # Get model prediction\n",
        "        logits = pl_module(val_imgs)\n",
        "        preds = torch.argmax(logits, -1)\n",
        "        # Log the images as wandb Image\n",
        "        trainer.logger.experiment.log({\n",
        "            \"examples\":[wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\")\n",
        "                           for x, pred, y in zip(val_imgs[:self.num_samples],\n",
        "                                                 preds[:self.num_samples],\n",
        "                                                 val_labels[:self.num_samples])]\n",
        "            })\n"
      ],
      "metadata": {
        "id": "X6ZqYrKHiZdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ncs757XoiZjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LitModel(pl.LightningModule):\n",
        "    def __init__(self, input_shape, num_classes, learning_rate=2e-4):\n",
        "        super().__init__()\n",
        "\n",
        "        # log hyperparameters\n",
        "        self.save_hyperparameters()\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, 1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.conv4 = nn.Conv2d(64, 64, 3, 1)\n",
        "\n",
        "        self.pool1 = torch.nn.MaxPool2d(2)\n",
        "        self.pool2 = torch.nn.MaxPool2d(2)\n",
        "\n",
        "        n_sizes = self._get_conv_output(input_shape)\n",
        "\n",
        "        self.fc1 = nn.Linear(n_sizes, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "        self.accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "\n",
        "    # returns the size of the output tensor going into Linear layer from the conv block.\n",
        "    def _get_conv_output(self, shape):\n",
        "        batch_size = 1\n",
        "        input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
        "\n",
        "        output_feat = self._forward_features(input)\n",
        "        n_size = output_feat.data.view(batch_size, -1).size(1)\n",
        "        return n_size\n",
        "\n",
        "    # returns the feature tensor from the conv block\n",
        "    def _forward_features(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool2(F.relu(self.conv4(x)))\n",
        "        return x\n",
        "\n",
        "    # will be used during inference\n",
        "    def forward(self, x):\n",
        "       x = self._forward_features(x)\n",
        "       x = x.view(x.size(0), -1)\n",
        "       x = F.relu(self.fc1(x))\n",
        "       x = F.relu(self.fc2(x))\n",
        "       x = F.log_softmax(self.fc3(x), dim=1)\n",
        "\n",
        "       return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.nll_loss(logits, y)\n",
        "\n",
        "        # training metrics\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = self.accuracy(preds, y)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
        "        self.log('train_acc', acc, on_step=True, on_epoch=True, logger=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.nll_loss(logits, y)\n",
        "\n",
        "        # validation metrics\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = self.accuracy(preds, y)\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_acc', acc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.nll_loss(logits, y)\n",
        "\n",
        "        # validation metrics\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = self.accuracy(preds, y)\n",
        "        self.log('test_loss', loss, prog_bar=True)\n",
        "        self.log('test_acc', acc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n"
      ],
      "metadata": {
        "id": "MS0j-7JviZpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BDkDzPGPiZuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dm = CIFAR10DataModule(batch_size=32)\n",
        "# To access the x_dataloader we need to call prepare_data and setup.\n",
        "dm.prepare_data()\n",
        "dm.setup()\n",
        "\n",
        "# Samples required by the custom ImagePredictionLogger callback to log image predictions.\n",
        "val_samples = next(iter(dm.val_dataloader()))\n",
        "val_imgs, val_labels = val_samples[0], val_samples[1]\n",
        "val_imgs.shape, val_labels.shape"
      ],
      "metadata": {
        "id": "QQhsu4QKiZ0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EK_BduEBiZ5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LitModel((3, 32, 32), dm.num_classes)\n",
        "\n",
        "# Initialize wandb logger\n",
        "wandb_logger = WandbLogger(project='wandb-lightning', job_type='train')\n",
        "\n",
        "# Initialize Callbacks\n",
        "early_stop_callback = pl.callbacks.EarlyStopping(monitor=\"val_loss\")\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint()\n",
        "\n",
        "# Initialize a trainer\n",
        "trainer = pl.Trainer(max_epochs=2,\n",
        "                     logger=wandb_logger,\n",
        "                     callbacks=[early_stop_callback,\n",
        "                                ImagePredictionLogger(val_samples),\n",
        "                                checkpoint_callback],\n",
        "                     )\n",
        "\n",
        "# Train the model âš¡ðŸš…âš¡\n",
        "trainer.fit(model, dm)\n",
        "\n",
        "# Evaluate the model on the held-out test set âš¡âš¡\n",
        "trainer.test(dataloaders=dm.test_dataloader())\n",
        "\n",
        "# Close wandb run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "mbgnWZTNiZ_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Huggingface"
      ],
      "metadata": {
        "id": "7J3jw9ztiaPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tMHW36Bsismd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets wandb evaluate accelerate -qU\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/text-classification/run_glue.py"
      ],
      "metadata": {
        "id": "o22jrvbHir24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JGjFx2tAivpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the run_glue.py script requires transformers dev\n",
        "!pip install -q git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "id": "868gnm2iivxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "92xBc60Uiv5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "QB6DIYZ7iv_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wUg66btSiwGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: log both gradients and parameters\n",
        "%env WANDB_WATCH=all"
      ],
      "metadata": {
        "id": "HdS2p3IhiwMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nu6034YfiwRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env WANDB_PROJECT=huggingface-demo\n",
        "%env TASK_NAME=MRPC\n",
        "\n",
        "!python run_glue.py \\\n",
        "  --model_name_or_path bert-base-uncased \\\n",
        "  --task_name $TASK_NAME \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 256 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-4 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir /tmp/$TASK_NAME/ \\\n",
        "  --overwrite_output_dir \\\n",
        "  --logging_steps 50"
      ],
      "metadata": {
        "id": "dHLo8djZiwV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorflow"
      ],
      "metadata": {
        "id": "8gsFygShj2ew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U5hETX15j2ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ARvzmTEYj2pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SBFKcdzYkjaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "I-aLIBs9kjgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uSNsTrw8kjmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "ZHYXz9fKkjsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cViZk6tPkjzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the training dataset\n",
        "BATCH_SIZE = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = np.reshape(x_train, (-1, 784))\n",
        "x_test = np.reshape(x_test, (-1, 784))\n",
        "\n",
        "# build input pipeline using tf.data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "zrLZHT-Ekj4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GHfXKknwkj92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model():\n",
        "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "    x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)\n",
        "    x2 = keras.layers.Dense(64, activation=\"relu\")(x1)\n",
        "    outputs = keras.layers.Dense(10, name=\"predictions\")(x2)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "1Yf-mRstkkCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Cx-DsKNYkkHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "\n",
        "    return loss_value"
      ],
      "metadata": {
        "id": "123o_YJnkkMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ghwcPifXkkQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(x, y, model, loss_fn, val_acc_metric):\n",
        "    val_logits = model(x, training=False)\n",
        "    loss_value = loss_fn(y, val_logits)\n",
        "    val_acc_metric.update_state(y, val_logits)\n",
        "\n",
        "    return loss_value"
      ],
      "metadata": {
        "id": "dE0P_mP6kkVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F4pk3CLpkkah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataset, val_dataset,  model, optimizer,\n",
        "          train_acc_metric, val_acc_metric,\n",
        "          epochs=10,  log_step=200, val_log_step=50):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "        train_loss = []\n",
        "        val_loss = []\n",
        "\n",
        "        # Iterate over the batches of the dataset\n",
        "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "            loss_value = train_step(x_batch_train, y_batch_train,\n",
        "                                    model, optimizer,\n",
        "                                    loss_fn, train_acc_metric)\n",
        "            train_loss.append(float(loss_value))\n",
        "\n",
        "        # Run a validation loop at the end of each epoch\n",
        "        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):\n",
        "            val_loss_value = test_step(x_batch_val, y_batch_val,\n",
        "                                       model, loss_fn,\n",
        "                                       val_acc_metric)\n",
        "            val_loss.append(float(val_loss_value))\n",
        "\n",
        "        # Display metrics at the end of each epoch\n",
        "        train_acc = train_acc_metric.result()\n",
        "        print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "        val_acc = val_acc_metric.result()\n",
        "        print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "\n",
        "        # Reset metrics at the end of each epoch\n",
        "        train_acc_metric.reset_states()\n",
        "        val_acc_metric.reset_states()\n",
        "\n",
        "        # â­: log metrics using wandb.log\n",
        "        wandb.log({'epochs': epoch,\n",
        "                   'loss': np.mean(train_loss),\n",
        "                   'acc': float(train_acc),\n",
        "                   'val_loss': np.mean(val_loss),\n",
        "                   'val_acc':float(val_acc)})"
      ],
      "metadata": {
        "id": "pCBVeQubkkfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ExHB3X5Wj2uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize wandb with your project name and optionally with configutations.\n",
        "# play around with the config values and see the result on your wandb dashboard.\n",
        "config = {\n",
        "              \"learning_rate\": 0.001,\n",
        "              \"epochs\": 10,\n",
        "              \"batch_size\": 64,\n",
        "              \"log_step\": 200,\n",
        "              \"val_log_step\": 50,\n",
        "              \"architecture\": \"CNN\",\n",
        "              \"dataset\": \"CIFAR-10\"\n",
        "           }\n",
        "\n",
        "run = wandb.init(project='my-tf-integration', config=config)\n",
        "config = wandb.config\n",
        "\n",
        "# Initialize model.\n",
        "model = make_model()\n",
        "\n",
        "# Instantiate an optimizer to train the model.\n",
        "optimizer = keras.optimizers.SGD(learning_rate=config.learning_rate)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "train(train_dataset,\n",
        "      val_dataset,\n",
        "      model,\n",
        "      optimizer,\n",
        "      train_acc_metric,\n",
        "      val_acc_metric,\n",
        "      epochs=config.epochs,\n",
        "      log_step=config.log_step,\n",
        "      val_log_step=config.val_log_step)\n",
        "\n",
        "run.finish()  # In Jupyter/Colab, let us know you're finished!"
      ],
      "metadata": {
        "id": "NdPYExVTj20C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras"
      ],
      "metadata": {
        "id": "UE7qucOfj3Eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RJdWt_bYj3HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq -U wandb"
      ],
      "metadata": {
        "id": "7WXiJzkSj3OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TW3ZakTYk9L8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Weights and Biases related imports\n",
        "import wandb\n",
        "from wandb.keras import WandbMetricsLogger"
      ],
      "metadata": {
        "id": "n72rujV1k9SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v-AYTzdVk9XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "WnoEmxvjk9kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4gCHhf9Xk9pL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configs = dict(\n",
        "    num_classes = 10,\n",
        "    shuffle_buffer = 1024,\n",
        "    batch_size = 64,\n",
        "    image_size = 28,\n",
        "    image_channels = 1,\n",
        "    earlystopping_patience = 3,\n",
        "    learning_rate = 1e-3,\n",
        "    epochs = 10\n",
        ")"
      ],
      "metadata": {
        "id": "itJvirh8k9uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EJHW9Ij4k9yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, valid_ds = tfds.load('fashion_mnist', split=['train', 'test'])"
      ],
      "metadata": {
        "id": "jwU-0yCOk928"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TUDLLrYIk97T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "\n",
        "def parse_data(example):\n",
        "    # Get image\n",
        "    image = example[\"image\"]\n",
        "    # image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "\n",
        "    # Get label\n",
        "    label = example[\"label\"]\n",
        "    label = tf.one_hot(label, depth=configs[\"num_classes\"])\n",
        "\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def get_dataloader(ds, configs, dataloader_type=\"train\"):\n",
        "    dataloader = ds.map(parse_data, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    if dataloader_type==\"train\":\n",
        "        dataloader = dataloader.shuffle(configs[\"shuffle_buffer\"])\n",
        "\n",
        "    dataloader = (\n",
        "        dataloader\n",
        "        .batch(configs[\"batch_size\"])\n",
        "        .prefetch(AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "L9R08Kyqk-AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zzrPuyyxk-E7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = get_dataloader(train_ds, configs)\n",
        "validloader = get_dataloader(valid_ds, configs, dataloader_type=\"valid\")"
      ],
      "metadata": {
        "id": "oSjftBCsk-LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H9ujZdXgk-QV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(configs):\n",
        "    backbone = tf.keras.applications.mobilenet_v2.MobileNetV2(weights='imagenet', include_top=False)\n",
        "    backbone.trainable = False\n",
        "\n",
        "    inputs = layers.Input(shape=(configs[\"image_size\"], configs[\"image_size\"], configs[\"image_channels\"]))\n",
        "    resize = layers.Resizing(32, 32)(inputs)\n",
        "    neck = layers.Conv2D(3, (3,3), padding=\"same\")(resize)\n",
        "    preprocess_input = tf.keras.applications.mobilenet.preprocess_input(neck)\n",
        "    x = backbone(preprocess_input)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    outputs = layers.Dense(configs[\"num_classes\"], activation=\"softmax\")(x)\n",
        "\n",
        "    return models.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "rvT2No1uk-br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1StRCdL3k-mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = get_model(configs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "on1BfHJsk-rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v0wZq1Uwk-v7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer = \"adam\",\n",
        "    loss = \"categorical_crossentropy\",\n",
        "    metrics = [\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top@5_accuracy')]\n",
        ")"
      ],
      "metadata": {
        "id": "1XxjLJb4k-0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wrI-Fi9-lLTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a W&B run\n",
        "run = wandb.init(\n",
        "    project = \"intro-keras\",\n",
        "    config = configs\n",
        ")\n",
        "\n",
        "# Train your model\n",
        "model.fit(\n",
        "    trainloader,\n",
        "    epochs = configs[\"epochs\"],\n",
        "    validation_data = validloader,\n",
        "    callbacks = [WandbMetricsLogger(log_freq=10)] # Notice the use of WandbMetricsLogger here\n",
        ")\n",
        "\n",
        "# Close the W&B run\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "SoY6OU4vlLY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost"
      ],
      "metadata": {
        "id": "e0m5PaUNj3ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PDDYHL47j58Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq wandb>=0.13.10 dill\n",
        "!pip install -qq xgboost>=1.7.4 scikit-learn>=1.2.1"
      ],
      "metadata": {
        "id": "JIPEbgoqj6Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hk5dGXeUlZhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "from dill.source import getsource\n",
        "from dill import detect\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.stats import ks_2samp\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "import xgboost as xgb\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)"
      ],
      "metadata": {
        "id": "Y5c_jSgblZqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "if0L3EhplZwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init()\n",
        "artifact = wandb.Artifact(\"mnist\", type=\"dataset\")\n",
        "artifact.add_reference(\"s3://my-bucket/datasets/mnist\")\n",
        "run.log_artifact(artifact)"
      ],
      "metadata": {
        "id": "yXaeIwf7lZ1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9x4PHG2xlZ6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "artifact = run.use_artifact(\"mnist:latest\", type=\"dataset\")\n",
        "artifact_dir = artifact.download()"
      ],
      "metadata": {
        "id": "gke0Wio_lZ-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lPFAUR0dlaNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "WANDB_PROJECT = \"vehicle_loan_default\""
      ],
      "metadata": {
        "id": "efl2s7_flaSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7VIjSdX2laW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specify a folder to save the data, a new folder will be created if it doesn't exist\n",
        "data_dir = Path(\".\")\n",
        "model_dir = Path(\"models\")\n",
        "model_dir.mkdir(exist_ok=True)\n",
        "\n",
        "id_vars = [\"UniqueID\"]\n",
        "targ_var = \"loan_default\""
      ],
      "metadata": {
        "id": "fF7SugG2labU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cJul4Mrwlafm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def function_to_string(fn):\n",
        "    return getsource(detect.code(fn))"
      ],
      "metadata": {
        "id": "sRGUUv0VlakH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JYxzqnColaok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(project=WANDB_PROJECT, job_type=\"preprocess-data\")"
      ],
      "metadata": {
        "id": "qgm8uHqzlasr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gVwU1SzIlayF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ARTIFACT_PATH = \"morgan/credit_scorecard/vehicle_loan_defaults:latest\"\n",
        "dataset_art = run.use_artifact(ARTIFACT_PATH, type=\"dataset\")\n",
        "dataset_dir = dataset_art.download(data_dir)"
      ],
      "metadata": {
        "id": "t0qPuWJcla2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9nkTug-Yla7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from data_utils import (\n",
        "    describe_data_g_targ,\n",
        "    one_hot_encode_data,\n",
        "    load_training_data,\n",
        ")"
      ],
      "metadata": {
        "id": "M2piFpH2lbAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "esdOI9qFlbEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data into Dataframe\n",
        "dataset = pd.read_csv(data_dir / \"vehicle_loans_subset.csv\")\n",
        "\n",
        "# One Hot Encode Data\n",
        "dataset, p_vars = one_hot_encode_data(dataset, id_vars, targ_var)\n",
        "\n",
        "# Save Preprocessed data\n",
        "processed_data_path = data_dir / \"proc_ds.csv\"\n",
        "dataset.to_csv(processed_data_path, index=False)"
      ],
      "metadata": {
        "id": "mu4ulDUSlbI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mBAzrOUcj6Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new artifact for the processed data, including the function that created it, to Artifacts\n",
        "processed_ds_art = wandb.Artifact(\n",
        "    name=\"vehicle_defaults_processed\",\n",
        "    type=\"processed_dataset\",\n",
        "    description=\"One-hot encoded dataset\",\n",
        "    metadata={\"preprocessing_fn\": function_to_string(one_hot_encode_data)},\n",
        ")\n",
        "\n",
        "# Attach our processed data to the Artifact\n",
        "processed_ds_art.add_file(processed_data_path)\n",
        "\n",
        "# Log this Artifact to the current wandb run\n",
        "run.log_artifact(processed_ds_art)\n",
        "\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "2sKAtgJUj6MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PiUGIxufj6Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(\n",
        "    project=WANDB_PROJECT, job_type=\"train-val-split\"\n",
        ") as run:  # config is optional here\n",
        "    # Download the subset of the vehicle loan default data from W&B\n",
        "    dataset_art = run.use_artifact(\n",
        "        \"vehicle_defaults_processed:latest\", type=\"processed_dataset\"\n",
        "    )\n",
        "    dataset_dir = dataset_art.download(data_dir)\n",
        "    dataset = pd.read_csv(processed_data_path)\n",
        "\n",
        "    # Set Split Params\n",
        "    test_size = 0.25\n",
        "    random_state = 42\n",
        "\n",
        "    # Log the splilt params\n",
        "    run.config.update({\"test_size\": test_size, \"random_state\": random_state})\n",
        "\n",
        "    # Do the Train/Val Split\n",
        "    trndat, valdat = model_selection.train_test_split(\n",
        "        dataset,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state,\n",
        "        stratify=dataset[[targ_var]],\n",
        "    )\n",
        "\n",
        "    print(f\"Train dataset size: {trndat[targ_var].value_counts()} \\n\")\n",
        "    print(f\"Validation dataset sizeL {valdat[targ_var].value_counts()}\")\n",
        "\n",
        "    # Save split datasets\n",
        "    train_path = data_dir / \"train.csv\"\n",
        "    val_path = data_dir / \"val.csv\"\n",
        "    trndat.to_csv(train_path, index=False)\n",
        "    valdat.to_csv(val_path, index=False)\n",
        "\n",
        "    # Create a new artifact for the processed data, including the function that created it, to Artifacts\n",
        "    split_ds_art = wandb.Artifact(\n",
        "        name=\"vehicle_defaults_split\",\n",
        "        type=\"train-val-dataset\",\n",
        "        description=\"Processed dataset split into train and valiation\",\n",
        "        metadata={\"test_size\": test_size, \"random_state\": random_state},\n",
        "    )\n",
        "\n",
        "    # Attach our processed data to the Artifact\n",
        "    split_ds_art.add_file(train_path)\n",
        "    split_ds_art.add_file(val_path)\n",
        "\n",
        "    # Log the Artifact\n",
        "    run.log_artifact(split_ds_art)"
      ],
      "metadata": {
        "id": "KYfpA9roj6U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GOL1dJwslubM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trndict = describe_data_g_targ(trndat, targ_var)\n",
        "trndat.head()"
      ],
      "metadata": {
        "id": "O3ad8UAPluhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OG3LnlePlum5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a wandb run, with an optional \"log-dataset\" job type to keep things tidy\n",
        "run = wandb.init(\n",
        "    project=WANDB_PROJECT, job_type=\"log-dataset\"\n",
        ")  # config is optional here\n",
        "\n",
        "# Create a W&B Table and log 1000 random rows of the dataset to explore\n",
        "table = wandb.Table(dataframe=trndat.sample(1000))\n",
        "\n",
        "# Log the Table to your W&B workspace\n",
        "wandb.log({\"processed_dataset\": table})\n",
        "\n",
        "# Close the wandb run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "SXNyFpzylurd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H_yENqzkluwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"tree_method\": \"gpu_hist\""
      ],
      "metadata": {
        "id": "awoMmElRlu1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YSyhQ915lu-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(project=WANDB_PROJECT, job_type=\"train-model\")"
      ],
      "metadata": {
        "id": "AKv7jET7lvJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4e_tgpj5lvOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_rate = round(trndict[\"base_rate\"], 6)\n",
        "early_stopping_rounds = 40"
      ],
      "metadata": {
        "id": "n-rYhPkalvTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "q1LgAqPelvYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bst_params = {\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"base_score\": base_rate,\n",
        "    \"gamma\": 1,  ## def: 0\n",
        "    \"learning_rate\": 0.1,  ## def: 0.1\n",
        "    \"max_depth\": 3,\n",
        "    \"min_child_weight\": 100,  ## def: 1\n",
        "    \"n_estimators\": 25,\n",
        "    \"nthread\": 24,\n",
        "    \"random_state\": 42,\n",
        "    \"reg_alpha\": 0,\n",
        "    \"reg_lambda\": 0,  ## def: 1\n",
        "    \"eval_metric\": [\"auc\", \"logloss\"],\n",
        "    \"tree_method\": \"hist\",  # use `gpu_hist` to train on GPU\n",
        "}"
      ],
      "metadata": {
        "id": "nlK83mO6lvde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DrMUsCcalvh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.config.update(dict(bst_params))\n",
        "run.config.update({\"early_stopping_rounds\": early_stopping_rounds})"
      ],
      "metadata": {
        "id": "_pSYaGvglvnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GpHwt7wPlvrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our training data from Artifacts\n",
        "trndat, valdat = load_training_data(\n",
        "    run=run, data_dir=data_dir, artifact_name=\"vehicle_defaults_split:latest\"\n",
        ")\n",
        "\n",
        "## Extract target column as a series\n",
        "y_trn = trndat.loc[:, targ_var].astype(int)\n",
        "y_val = valdat.loc[:, targ_var].astype(int)"
      ],
      "metadata": {
        "id": "de1eBy4jlvwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mPXthy_Ilv1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wandb.xgboost import WandbCallback\n",
        "\n",
        "# Initialize the XGBoostClassifier with the WandbCallback\n",
        "xgbmodel = xgb.XGBClassifier(\n",
        "    **bst_params,\n",
        "    callbacks=[WandbCallback(log_model=True)],\n",
        "    early_stopping_rounds=run.config[\"early_stopping_rounds\"]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgbmodel.fit(trndat[p_vars], y_trn, eval_set=[(valdat[p_vars], y_val)])"
      ],
      "metadata": {
        "id": "ywSQRWgUlv5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KFm3M35il747"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bstr = xgbmodel.get_booster()\n",
        "\n",
        "# Get train and validation predictions\n",
        "trnYpreds = xgbmodel.predict_proba(trndat[p_vars])[:, 1]\n",
        "valYpreds = xgbmodel.predict_proba(valdat[p_vars])[:, 1]\n",
        "\n",
        "# Log additional Train metrics\n",
        "false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(\n",
        "    y_trn, trnYpreds\n",
        ")\n",
        "run.summary[\"train_ks_stat\"] = max(true_positive_rate - false_positive_rate)\n",
        "run.summary[\"train_auc\"] = metrics.auc(false_positive_rate, true_positive_rate)\n",
        "run.summary[\"train_log_loss\"] = -(\n",
        "    y_trn * np.log(trnYpreds) + (1 - y_trn) * np.log(1 - trnYpreds)\n",
        ").sum() / len(y_trn)\n",
        "\n",
        "# Log additional Validation metrics\n",
        "ks_stat, ks_pval = ks_2samp(valYpreds[y_val == 1], valYpreds[y_val == 0])\n",
        "run.summary[\"val_ks_2samp\"] = ks_stat\n",
        "run.summary[\"val_ks_pval\"] = ks_pval\n",
        "run.summary[\"val_auc\"] = metrics.roc_auc_score(y_val, valYpreds)\n",
        "run.summary[\"val_acc_0.5\"] = metrics.accuracy_score(\n",
        "    y_val, np.where(valYpreds >= 0.5, 1, 0)\n",
        ")\n",
        "run.summary[\"val_log_loss\"] = -(\n",
        "    y_val * np.log(valYpreds) + (1 - y_val) * np.log(1 - valYpreds)\n",
        ").sum() / len(y_val)"
      ],
      "metadata": {
        "id": "cy2AISrnl7-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MrZ_5hgUl8EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log the ROC curve to W&B\n",
        "valYpreds_2d = np.array([1 - valYpreds, valYpreds])  # W&B expects a 2d array\n",
        "y_val_arr = y_val.values\n",
        "d = 0\n",
        "while len(valYpreds_2d.T) > 10000:\n",
        "    d += 1\n",
        "    valYpreds_2d = valYpreds_2d[::1, ::d]\n",
        "    y_val_arr = y_val_arr[::d]\n",
        "\n",
        "run.log(\n",
        "    {\n",
        "        \"ROC_Curve\": wandb.plot.roc_curve(\n",
        "            y_val_arr,\n",
        "            valYpreds_2d.T,\n",
        "            labels=[\"no_default\", \"loan_default\"],\n",
        "            classes_to_plot=[1],\n",
        "        )\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "_By9yMXdl8Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jGDOir9dl8OU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.finish()"
      ],
      "metadata": {
        "id": "PvCEOsFOl8T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cqWIUCMzl8ZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \"method\": \"random\",\n",
        "    \"parameters\": {\n",
        "        \"learning_rate\": {\"min\": 0.001, \"max\": 1.0},\n",
        "        \"gamma\": {\"min\": 0.001, \"max\": 1.0},\n",
        "        \"min_child_weight\": {\"min\": 1, \"max\": 150},\n",
        "        \"early_stopping_rounds\": {\"values\": [10, 20, 30, 40]},\n",
        "    },\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=WANDB_PROJECT)"
      ],
      "metadata": {
        "id": "eg-ju_Q2l8eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SOv7fFxbl8jX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    with wandb.init(job_type=\"sweep\") as run:\n",
        "        bst_params = {\n",
        "            \"objective\": \"binary:logistic\",\n",
        "            \"base_score\": base_rate,\n",
        "            \"gamma\": run.config[\"gamma\"],\n",
        "            \"learning_rate\": run.config[\"learning_rate\"],\n",
        "            \"max_depth\": 3,\n",
        "            \"min_child_weight\": run.config[\"min_child_weight\"],\n",
        "            \"n_estimators\": 25,\n",
        "            \"nthread\": 24,\n",
        "            \"random_state\": 42,\n",
        "            \"reg_alpha\": 0,\n",
        "            \"reg_lambda\": 0,  ## def: 1\n",
        "            \"eval_metric\": [\"auc\", \"logloss\"],\n",
        "            \"tree_method\": \"hist\",\n",
        "        }\n",
        "\n",
        "        # Initialize the XGBoostClassifier with the WandbCallback\n",
        "        xgbmodel = xgb.XGBClassifier(\n",
        "            **bst_params,\n",
        "            callbacks=[WandbCallback()],\n",
        "            early_stopping_rounds=run.config[\"early_stopping_rounds\"]\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        xgbmodel.fit(trndat[p_vars], y_trn, eval_set=[(valdat[p_vars], y_val)])\n",
        "\n",
        "        bstr = xgbmodel.get_booster()\n",
        "\n",
        "        # Log booster metrics\n",
        "        run.summary[\"best_ntree_limit\"] = bstr.best_ntree_limit\n",
        "\n",
        "        # Get train and validation predictions\n",
        "        trnYpreds = xgbmodel.predict_proba(trndat[p_vars])[:, 1]\n",
        "        valYpreds = xgbmodel.predict_proba(valdat[p_vars])[:, 1]\n",
        "\n",
        "        # Log additional Train metrics\n",
        "        false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(\n",
        "            y_trn, trnYpreds\n",
        "        )\n",
        "        run.summary[\"train_ks_stat\"] = max(true_positive_rate - false_positive_rate)\n",
        "        run.summary[\"train_auc\"] = metrics.auc(false_positive_rate, true_positive_rate)\n",
        "        run.summary[\"train_log_loss\"] = -(\n",
        "            y_trn * np.log(trnYpreds) + (1 - y_trn) * np.log(1 - trnYpreds)\n",
        "        ).sum() / len(y_trn)\n",
        "\n",
        "        # Log additional Validation metrics\n",
        "        ks_stat, ks_pval = ks_2samp(valYpreds[y_val == 1], valYpreds[y_val == 0])\n",
        "        run.summary[\"val_ks_2samp\"] = ks_stat\n",
        "        run.summary[\"val_ks_pval\"] = ks_pval\n",
        "        run.summary[\"val_auc\"] = metrics.roc_auc_score(y_val, valYpreds)\n",
        "        run.summary[\"val_acc_0.5\"] = metrics.accuracy_score(\n",
        "            y_val, np.where(valYpreds >= 0.5, 1, 0)\n",
        "        )\n",
        "        run.summary[\"val_log_loss\"] = -(\n",
        "            y_val * np.log(valYpreds) + (1 - y_val) * np.log(1 - valYpreds)\n",
        "        ).sum() / len(y_val)"
      ],
      "metadata": {
        "id": "DsaGT2Cgl8ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g_R3hn-Ql8tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 10  # number of runs to execute\n",
        "wandb.agent(sweep_id, function=train, count=count)"
      ],
      "metadata": {
        "id": "F4_SNo0fl8yG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LightGBM"
      ],
      "metadata": {
        "id": "wxVyIQElkVtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "id_SRFnVkV3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq 'lightgbm>=3.3.1'"
      ],
      "metadata": {
        "id": "zQORiYUdkdFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LBtaZ_xqkdJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "GLVlYuE2kdPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8Q1DdTsikdXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU wandb"
      ],
      "metadata": {
        "id": "btwzsNe1kdcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pU87iwFtkdgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from wandb.lightgbm import wandb_callback, log_summary\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "CarPimogkdk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Wlf7ycVakdp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/microsoft/LightGBM/master/examples/regression/regression.train -qq\n",
        "!wget https://raw.githubusercontent.com/microsoft/LightGBM/master/examples/regression/regression.test -qq"
      ],
      "metadata": {
        "id": "zDqKCtiCkdva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oKQpn2k1keOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load or create your dataset\n",
        "df_train = pd.read_csv(\"regression.train\", header=None, sep=\"\\t\")\n",
        "df_test = pd.read_csv(\"regression.test\", header=None, sep=\"\\t\")\n",
        "\n",
        "y_train = df_train[0]\n",
        "y_test = df_test[0]\n",
        "X_train = df_train.drop(0, axis=1)\n",
        "X_test = df_test.drop(0, axis=1)\n",
        "\n",
        "# create dataset for lightgbm\n",
        "lgb_train = lgb.Dataset(X_train, y_train)\n",
        "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)"
      ],
      "metadata": {
        "id": "Oz2iSa7CkV7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jNDXPMU2mXu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specify your configurations as a dict\n",
        "params = {\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"objective\": \"regression\",\n",
        "    \"metric\": [\"rmse\", \"l2\", \"l1\", \"huber\"],\n",
        "    \"num_leaves\": 31,\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"feature_fraction\": 0.9,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 5,\n",
        "    \"verbosity\": 0,\n",
        "}\n",
        "\n",
        "wandb.init(project=\"my-lightgbm-project\", config=params)"
      ],
      "metadata": {
        "id": "C6ObIAGcmX2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lEfzIiS3mX7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "# add lightgbm callback\n",
        "gbm = lgb.train(\n",
        "    params,\n",
        "    lgb_train,\n",
        "    num_boost_round=30,\n",
        "    valid_sets=lgb_eval,\n",
        "    valid_names=(\"validation\"),\n",
        "    callbacks=[wandb_callback()],\n",
        "    early_stopping_rounds=5,\n",
        ")"
      ],
      "metadata": {
        "id": "w3tbno1xmYA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mN5DRygAmYFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_summary(gbm, save_model_checkpoint=True)"
      ],
      "metadata": {
        "id": "ZKR8YsPamYKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SssDfcbKmYPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict\n",
        "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
        "\n",
        "# eval\n",
        "print(\"The rmse of prediction is:\", mean_squared_error(y_test, y_pred) ** 0.5)\n",
        "wandb.log({\"rmse_prediction\": mean_squared_error(y_test, y_pred) ** 0.5})"
      ],
      "metadata": {
        "id": "auWlqR3imYUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TZ5tbXMnmYY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "uffaZW3hmYdl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}